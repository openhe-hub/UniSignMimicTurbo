# FramerTurbo LoRA Fine-tuning Guide

è¿™æ˜¯ FramerTurbo çš„ LoRA å¾®è°ƒè®­ç»ƒä»£ç ã€‚æ”¯æŒåœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šè¿›è¡Œé«˜æ•ˆå¾®è°ƒã€‚

## ğŸ“‹ ç‰¹æ€§

- âœ… **LoRA é«˜æ•ˆå¾®è°ƒ**: ä½¿ç”¨ PEFT åº“ï¼Œæ˜¾å­˜å‹å¥½ï¼ˆ~16-24GBï¼‰
- âœ… **å¤šç§æ•°æ®æ ¼å¼**: æ”¯æŒè§†é¢‘æ–‡ä»¶æˆ–å›¾åƒå¯¹
- âœ… **æ··åˆç²¾åº¦è®­ç»ƒ**: æ”¯æŒ FP16/BF16
- âœ… **æ¢¯åº¦ç´¯ç§¯**: æ”¯æŒå°æ˜¾å­˜è®­ç»ƒ
- âœ… **Accelerate é›†æˆ**: æ”¯æŒå•å¡/å¤šå¡è®­ç»ƒ
- âœ… **çµæ´»é…ç½®**: å¯é€‰è®­ç»ƒ UNet å’Œ/æˆ– ControlNet

## ğŸš€ å¿«é€Ÿå¼€å§‹

> **é‡è¦**: è¯·ä» FramerTurbo é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ‰€æœ‰å‘½ä»¤ï¼

### 1. å®‰è£…ä¾èµ–

```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹
pip install -r requirements.txt
pip install accelerate peft wandb
```

### 2. å‡†å¤‡æ•°æ®é›†

#### æ–¹å¼ A: è§†é¢‘æ–‡ä»¶ï¼ˆæ¨èï¼‰

å°†è§†é¢‘æ–‡ä»¶æ”¾åœ¨ä¸€ä¸ªç›®å½•ä¸‹ï¼š

```
data/training_videos/
    video_001.mp4
    video_002.mp4
    video_003.mp4
    ...
```

#### æ–¹å¼ B: å›¾åƒå¯¹

å°†èµ·å§‹å¸§å’Œç»“æŸå¸§é…å¯¹ï¼š

```
data/image_pairs/
    sample_001_start.jpg
    sample_001_end.jpg
    sample_002_start.jpg
    sample_002_end.jpg
    ...
```

### 3. é…ç½®è®­ç»ƒè„šæœ¬

ç¼–è¾‘ `scripts/train_lora.sh`:

```bash
# ä¿®æ”¹æ•°æ®è·¯å¾„
DATA_DIR="data/training_videos"  # ä½ çš„æ•°æ®ç›®å½•

# é€‰æ‹©æ•°æ®é›†ç±»å‹
DATASET_TYPE="video"  # æˆ– "image_pair"

# è°ƒæ•´è®­ç»ƒå‚æ•°
BATCH_SIZE=1          # æ ¹æ®æ˜¾å­˜è°ƒæ•´
GRADIENT_ACCUM=4      # æœ‰æ•ˆ batch size = BATCH_SIZE Ã— GRADIENT_ACCUM
EPOCHS=10             # è®­ç»ƒè½®æ•°
LEARNING_RATE=1e-4    # å­¦ä¹ ç‡
```

### 4. å¯åŠ¨è®­ç»ƒ

```bash
cd FramerTurbo
bash scripts/train_lora.sh
```

## âš™ï¸ è®­ç»ƒå‚æ•°è¯´æ˜

### æ ¸å¿ƒå‚æ•°

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `--pretrained_model_path` | é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ | `checkpoints/framer_512x320` |
| `--data_dir` | è®­ç»ƒæ•°æ®ç›®å½• | - |
| `--output_dir` | è¾“å‡ºç›®å½• | - |
| `--dataset_type` | æ•°æ®é›†ç±»å‹: `video` æˆ– `image_pair` | `video` |

### LoRA å‚æ•°

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| `--lora_rank` | LoRA ç§©ï¼ˆè¶Šå¤§æ•ˆæœè¶Šå¥½ä½†æ˜¾å­˜è¶Šå¤šï¼‰ | 64 |
| `--lora_alpha` | LoRA alphaï¼ˆé€šå¸¸ç­‰äº rankï¼‰ | 64 |
| `--lora_dropout` | LoRA dropout | 0.0 |
| `--train_unet` | è®­ç»ƒ UNetï¼ˆå¿…é€‰ï¼‰ | âœ“ |
| `--train_controlnet` | è®­ç»ƒ ControlNetï¼ˆå¯é€‰ï¼‰ | - |

### è®­ç»ƒè¶…å‚æ•°

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| `--train_batch_size` | æ¯å¡ batch size | 1 |
| `--gradient_accumulation_steps` | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° | 4-8 |
| `--num_train_epochs` | è®­ç»ƒè½®æ•° | 10-20 |
| `--learning_rate` | å­¦ä¹ ç‡ | 1e-4 |
| `--mixed_precision` | æ··åˆç²¾åº¦: `fp16` æˆ– `bf16` | `fp16` |

## ğŸ’¾ æ˜¾å­˜éœ€æ±‚

| é…ç½® | æ˜¾å­˜éœ€æ±‚ | è¯´æ˜ |
|------|----------|------|
| LoRA (rank=64) + UNet | ~16-20 GB | æ¨èé…ç½® |
| LoRA (rank=128) + UNet | ~20-24 GB | æ›´é«˜è´¨é‡ |
| LoRA + UNet + ControlNet | ~24-32 GB | å®Œæ•´è®­ç»ƒ |
| å…¨é‡å¾®è°ƒ | ~40+ GB | æœ€ä½³æ•ˆæœ |

**èŠ‚çœæ˜¾å­˜æŠ€å·§**:
- å‡å° `--lora_rank` (å¦‚ 32)
- å‡å° `--train_batch_size` å¹¶å¢åŠ  `--gradient_accumulation_steps`
- ä½¿ç”¨ `--mixed_precision fp16`
- å‡å° `--num_frames` (å¦‚ 3 â†’ 2)

## ğŸ“Š ç›‘æ§è®­ç»ƒ

### TensorBoard

```bash
tensorboard --logdir logs
```

### Weights & Biases

```bash
# åœ¨è®­ç»ƒè„šæœ¬ä¸­æ·»åŠ 
USE_WANDB="--use_wandb"
```

## ğŸ”„ ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹

### åŠ è½½ LoRA æƒé‡

```python
from peft import PeftModel
from models_diffusers.unet_spatio_temporal_condition import UNetSpatioTemporalConditionModel

# åŠ è½½åŸºç¡€æ¨¡å‹
unet = UNetSpatioTemporalConditionModel.from_pretrained(
    "checkpoints/framer_512x320/unet",
    torch_dtype=torch.float16,
)

# åŠ è½½ LoRA æƒé‡
unet = PeftModel.from_pretrained(
    unet,
    "outputs/lora_finetune/final/unet_lora",
)

# åˆå¹¶ LoRAï¼ˆå¯é€‰ï¼Œç”¨äºæ¨ç†åŠ é€Ÿï¼‰
unet = unet.merge_and_unload()
```

### æ¨ç†ç¤ºä¾‹

```python
# å°†å¾®è°ƒåçš„ UNet é›†æˆåˆ°æ¨ç† pipeline
pipe = StableVideoDiffusionInterpControlPipeline.from_pretrained(
    "checkpoints/stable-video-diffusion-img2vid-xt",
    unet=unet,  # ä½¿ç”¨å¾®è°ƒåçš„ UNet
    controlnet=controlnet,
    torch_dtype=torch.float16,
)

# æ­£å¸¸æ¨ç†
frames = pipe(
    first_image,
    last_image,
    num_frames=3,
    ...
).frames
```

## ğŸ› ï¸ é«˜çº§ç”¨æ³•

### å¤šå¡è®­ç»ƒ

ä½¿ç”¨ Accelerate é…ç½®æ–‡ä»¶:

```bash
accelerate config  # é…ç½®å¤šå¡è®¾ç½®
accelerate launch training/train_lora.py ...  # è‡ªåŠ¨å¤šå¡è®­ç»ƒ
```

### ä»æ£€æŸ¥ç‚¹æ¢å¤

```bash
python training/train_lora.py \
  --resume_from_checkpoint outputs/lora_finetune/checkpoint-1000 \
  ...
```

### ä»…è®­ç»ƒ ControlNet

```bash
python training/train_lora.py \
  --train_controlnet \  # åªè®­ç»ƒ ControlNet
  --learning_rate 5e-5 \  # ControlNet å»ºè®®æ›´å°çš„å­¦ä¹ ç‡
  ...
```

### æ··åˆè®­ç»ƒï¼ˆUNet LoRA + ControlNet å…¨é‡ï¼‰

```bash
python training/train_lora.py \
  --train_unet \
  --train_controlnet \
  --lora_rank 64 \
  ...
```

## ğŸ“ è¾“å‡ºç»“æ„

```
outputs/lora_finetune/
â”œâ”€â”€ checkpoint-500/
â”‚   â””â”€â”€ unet_lora/          # LoRA æƒé‡æ£€æŸ¥ç‚¹
â”œâ”€â”€ checkpoint-1000/
â”‚   â””â”€â”€ unet_lora/
â””â”€â”€ final/
    â””â”€â”€ unet_lora/          # æœ€ç»ˆ LoRA æƒé‡
        â”œâ”€â”€ adapter_config.json
        â””â”€â”€ adapter_model.safetensors
```

## â“ å¸¸è§é—®é¢˜

### Q: è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¾å­˜æº¢å‡ºæ€ä¹ˆåŠï¼Ÿ

A: å°è¯•ä»¥ä¸‹æ–¹æ³•ï¼š
1. å‡å° `--train_batch_size` è‡³ 1
2. å¢åŠ  `--gradient_accumulation_steps` è‡³ 8 æˆ–æ›´é«˜
3. å‡å° `--lora_rank` è‡³ 32 æˆ– 16
4. å‡å° `--num_frames` è‡³ 2

### Q: è®­ç»ƒå¤šå°‘æ­¥åˆé€‚ï¼Ÿ

A: å–å†³äºæ•°æ®é›†å¤§å°ï¼š
- å°æ•°æ®é›† (< 100 è§†é¢‘): 10-20 epochs
- ä¸­æ•°æ®é›† (100-1000 è§†é¢‘): 5-10 epochs
- å¤§æ•°æ®é›† (> 1000 è§†é¢‘): 2-5 epochs

å»ºè®®æ¯ 500 æ­¥ä¿å­˜æ£€æŸ¥ç‚¹ï¼Œæ ¹æ®éªŒè¯æ•ˆæœé€‰æ‹©æœ€ä½³æ¨¡å‹ã€‚

### Q: å¦‚ä½•è°ƒæ•´å­¦ä¹ ç‡ï¼Ÿ

A: å‚è€ƒå»ºè®®ï¼š
- LoRA è®­ç»ƒ: `1e-4` åˆ° `5e-5`
- ControlNet è®­ç»ƒ: `5e-5` åˆ° `1e-5`
- å¦‚æœ loss ä¸ä¸‹é™ï¼Œå°è¯•æé«˜å­¦ä¹ ç‡
- å¦‚æœ loss éœ‡è¡ï¼Œå°è¯•é™ä½å­¦ä¹ ç‡

### Q: æ”¯æŒè‡ªå®šä¹‰è½¨è¿¹æ ‡æ³¨å—ï¼Ÿ

A: å½“å‰ç‰ˆæœ¬çš„è®­ç»ƒä»£ç æš‚æœªé›†æˆè½¨è¿¹ç‚¹æ ‡æ³¨ã€‚å¦‚éœ€è®­ç»ƒè½¨è¿¹æ§åˆ¶èƒ½åŠ›ï¼Œéœ€è¦ï¼š
1. å‡†å¤‡å¸¦è½¨è¿¹æ ‡æ³¨çš„æ•°æ®é›†
2. ä¿®æ”¹ `train_dataset.py` åŠ è½½è½¨è¿¹æ•°æ®
3. åœ¨ `train_lora.py` ä¸­æ·»åŠ  ControlNet æ¡ä»¶

æˆ‘ä»¬è®¡åˆ’åœ¨åç»­ç‰ˆæœ¬ä¸­æ·»åŠ å®Œæ•´çš„è½¨è¿¹æ ‡æ³¨è®­ç»ƒæ”¯æŒã€‚

## ğŸ“š å‚è€ƒèµ„æ–™

- [LoRA è®ºæ–‡](https://arxiv.org/abs/2106.09685)
- [PEFT æ–‡æ¡£](https://huggingface.co/docs/peft)
- [Accelerate æ–‡æ¡£](https://huggingface.co/docs/accelerate)
- [FramerTurbo è®ºæ–‡](https://arxiv.org/abs/2410.18978)

## ğŸ“ è®¸å¯è¯

æœ¬è®­ç»ƒä»£ç éµå¾ª FramerTurbo çš„è®¸å¯è¯ã€‚
